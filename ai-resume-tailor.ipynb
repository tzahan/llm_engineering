{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#pip install gradio transformers sentence-transformers torch\n!pip install -q requests torch bitsandbytes transformers sentencepiece accelerate  sentence-transformers gradio","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"WciV7Vy3Affw","outputId":"8c68cb8f-d3ed-42d4-bbfa-a6ab54340642","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:30:29.427976Z","iopub.execute_input":"2025-10-09T08:30:29.428158Z","iopub.status.idle":"2025-10-09T08:31:57.832005Z","shell.execute_reply.started":"2025-10-09T08:30:29.428136Z","shell.execute_reply":"2025-10-09T08:31:57.831332Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.6/68.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.3/564.3 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nthinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport requests\nimport json\nfrom google.colab import userdata\n\nfrom huggingface_hub import login\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\n\nimport gradio as gr","metadata":{"id":"UaYw92_9A-QG","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:31:57.835652Z","iopub.execute_input":"2025-10-09T08:31:57.836273Z","iopub.status.idle":"2025-10-09T08:32:40.073549Z","shell.execute_reply.started":"2025-10-09T08:31:57.836249Z","shell.execute_reply":"2025-10-09T08:32:40.072958Z"}},"outputs":[{"name":"stderr","text":"2025-10-09 08:32:16.623969: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1759998737.028140      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1759998737.159486      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from google.colab import drive\ndrive.mount('/content/drive')","metadata":{"id":"kluBPMmLIRkp","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a cache folder on Drive\ncache_dir = \"/content/drive/MyDrive/huggingface\"\nos.makedirs(cache_dir, exist_ok=True)\n\n# Set environment variable for Transformers cache\nos.environ[\"TRANSFORMERS_CACHE\"] = cache_dir","metadata":{"id":"_bJQhImNIUiP"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## HuggingFace models","metadata":{"id":"KnuQ9SdHIZ6S"}},{"cell_type":"code","source":"# Sign in to HuggingFace Hub\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nif not hf_token:\n    raise ValueError(\"Missing Hugging Face token in Colab secrets.\")\n#login(token=hf_token)\nlogin(hf_token, add_to_git_credential=True)","metadata":{"id":"RvNCsz_qIWsh","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:35:55.099331Z","iopub.execute_input":"2025-10-09T08:35:55.100626Z","iopub.status.idle":"2025-10-09T08:35:55.448633Z","shell.execute_reply.started":"2025-10-09T08:35:55.100598Z","shell.execute_reply":"2025-10-09T08:35:55.447887Z"}},"outputs":[{"name":"stderr","text":"Token has not been saved to git credential helper.\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\nYou might have to re-authenticate when pushing to the Hugging Face Hub.\nRun the following command in your terminal in case you want to set the 'store' credential helper as default.\n\ngit config --global credential.helper store\n\nRead https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import whoami\nprint(whoami())","metadata":{"id":"IeCT8lj4IjPZ","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nrequests.get(\"https://huggingface.co\").status_code","metadata":{"id":"mfTL1Xqgarp_","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:36:45.599605Z","iopub.execute_input":"2025-10-09T08:36:45.600327Z","iopub.status.idle":"2025-10-09T08:36:45.734917Z","shell.execute_reply.started":"2025-10-09T08:36:45.600297Z","shell.execute_reply":"2025-10-09T08:36:45.734347Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"200"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"# Embedding model\nembed_model_name = \"sentence-transformers/all-mpnet-base-v2\"\n# embed_model_name = \"sentence-transformers/all-MiniLM-L12-v2\"\n# embed_model_name = \"paraphrase-multilingual-mpnet-base-v2\"\n# embed_model_name = \"hkunlp/instructor-large\"  # requires special usage below\n\nembedder = SentenceTransformer(embed_model_name, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Rewriting model\nllama_model = 'meta-llama/Meta-Llama-3.1-8B-Instruct'\ndeepseek_model = 'deepseek-ai/deepseek-llm-7b-chat'\nqwen2 = 'Qwen/Qwen2-7B-Instruct'\n\n#selected model\nmodel_id = llama_model","metadata":{"id":"H1qmLpi_ImoZ","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:07:39.977003Z","iopub.execute_input":"2025-10-09T09:07:39.977342Z","iopub.status.idle":"2025-10-09T09:07:45.794695Z","shell.execute_reply.started":"2025-10-09T09:07:39.977319Z","shell.execute_reply":"2025-10-09T09:07:45.794119Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8edf14da0e4e4ac9bf89983dc73da31f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a973f5fee7724247b3e44ff19d5aab33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"02c375cd7f6e47a099231577e1b345a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3354db1add84560a4e12a0d7299de80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2ff5e23633147e691871a90072e2eb8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13c6d10ada544d84b41c78551eb2b27a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b421915ae21d40639a3289c87345b0fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f47788b49f641c3bad755549f817f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a492945cd4a1445285c9142abee8862b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c9a0768e2cb445fa3a156437a9195eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e32aa3be9b45f6a7250da0a2259a1a"}},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"### Creating Prompts","metadata":{"id":"bf5Yo50WJCIq"}},{"cell_type":"code","source":"system_prompt = f\"\"\"You are an expert CV writing assistant.\nYour goal is to generate tailored CV based on a given CV and job description.\nFocus on matching key skills, tone, and phrasing, but keep the resume truthful.\nDo not repeat the instructions.\"\"\"","metadata":{"id":"fdSjSuTkI227","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:38:02.503191Z","iopub.execute_input":"2025-10-09T08:38:02.503903Z","iopub.status.idle":"2025-10-09T08:38:02.507270Z","shell.execute_reply.started":"2025-10-09T08:38:02.503881Z","shell.execute_reply":"2025-10-09T08:38:02.506515Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def complete_user_prompt(cv_text, job_description):\n    user_prompt = f\"\"\"Please Generate a recruiter friendly CV for the following job:\n      Job Description:\n      {job_description}\n\n      Resume Section:\n      {cv_text}\n      \"\"\"\n    messages = [\n        {'role': 'system', 'content': system_prompt},\n        {'role': 'user', 'content': user_prompt}\n    ]\n\n    return messages","metadata":{"id":"EwzjuOl-Qzda","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:38:04.166620Z","iopub.execute_input":"2025-10-09T08:38:04.166899Z","iopub.status.idle":"2025-10-09T08:38:04.171135Z","shell.execute_reply.started":"2025-10-09T08:38:04.166879Z","shell.execute_reply":"2025-10-09T08:38:04.170420Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Accessing the Models","metadata":{"id":"ZQgYIybQRhxX"}},{"cell_type":"code","source":"print(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"GPU-Device:\", torch.cuda.get_device_name(torch.cuda.current_device()))\nelse:\n    print(\"No GPU found.\")","metadata":{"id":"BYSoTE90RtwA","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:38:10.799749Z","iopub.execute_input":"2025-10-09T08:38:10.800263Z","iopub.status.idle":"2025-10-09T08:38:10.804915Z","shell.execute_reply.started":"2025-10-09T08:38:10.800238Z","shell.execute_reply":"2025-10-09T08:38:10.804156Z"}},"outputs":[{"name":"stdout","text":"CUDA available: True\nGPU-Device: Tesla T4\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# Quantization Config\nquant_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_use_double_quant = True,\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_quant_type= 'nf4'\n)\n\n# Load tokenizer + model\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code = True)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    #cache_dir=cache_dir,\n    quantization_config=quant_config,\n)","metadata":{"id":"sLQcyBNRRvCh","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:39:04.506783Z","iopub.execute_input":"2025-10-09T08:39:04.507466Z","iopub.status.idle":"2025-10-09T08:42:11.779473Z","shell.execute_reply.started":"2025-10-09T08:39:04.507433Z","shell.execute_reply":"2025-10-09T08:42:11.776834Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c32817b30e964917affe33a35435daf0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38379d78f9c746ae978f89319cf53878"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"433c2202d4be4377baa36bdc57a1eebd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67ef9d1e9ae0494692b83837be877774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8179964702f4691bd0694330c6c2241"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b10342040f547b9a5e20d19216a51d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5869651f410b4add858d55ac01f94f55"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"488a2c6a14d844448b120f027e9d9da0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb0fe9a9b80d4edaa531ecfcc58ff893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e1c63795494031b87fceb8c33bf52d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e83ab01f6ea448e8e74c104ddbfdc13"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b63c690d0ed34a19acc313312c2da722"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"def rewrite_cv(cv_text, job_description):\n    try:\n        messages = complete_user_prompt(cv_text, job_description)\n        # Format messages correctly for chat template\n        inputs = tokenizer.apply_chat_template(messages, return_tensors='pt').to('cuda')\n\n        outputs = model.generate(\n            inputs,\n            max_new_tokens=2000,\n            temperature=0.7,\n            top_p=0.9,\n            #streamer=streamer,  # remove if you don’t want live output\n        )\n\n        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n        #del tokenizer, model, inputs, outputs #, streamer\n        return generated_text.split(messages[-1][\"content\"])[-1].strip()\n\n    except Exception as e:\n        return f\"Error during generation: {str(e)}\"\n","metadata":{"id":"tlcK1xqlR2qZ","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T08:42:22.251308Z","iopub.execute_input":"2025-10-09T08:42:22.252077Z","iopub.status.idle":"2025-10-09T08:42:22.260110Z","shell.execute_reply.started":"2025-10-09T08:42:22.252044Z","shell.execute_reply":"2025-10-09T08:42:22.259306Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def compute_similarity(cv_text, job_description):\n    # Compute similarity score\n    cv_emb = embedder.encode(cv_text, convert_to_tensor=True)\n    job_emb = embedder.encode(job_description, convert_to_tensor=True)\n    score = float(torch.nn.functional.cosine_similarity(cv_emb, job_emb, dim=0))\n\n    return round(score * 100, 2)\n    # Rewrite resume using LLaMA\n    #rewritten = rewrite_cv(cv_text, job_description)\n\n    #return f\"**Semantic Match Score:** {similarity:.2f}\\n\\n**Rewritten Resume:**\\n{rewritten}\"","metadata":{"id":"r-2F_DwOB3Y8","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:07:54.822666Z","iopub.execute_input":"2025-10-09T09:07:54.823212Z","iopub.status.idle":"2025-10-09T09:07:54.827549Z","shell.execute_reply.started":"2025-10-09T09:07:54.823188Z","shell.execute_reply":"2025-10-09T09:07:54.826894Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"## Gradio UI","metadata":{}},{"cell_type":"code","source":"with gr.Blocks() as app:\n    gr.Markdown(\"## 🧠 AI Resume Tailor — Powered by Hugging Face Transformers\")\n\n    with gr.Row():\n        cv_input = gr.Textbox(label=\"Paste Your CV\", lines=10, placeholder=\"Enter CV text...\")\n        job_input = gr.Textbox(label=\"Paste Job Description\", lines=10, placeholder=\"Enter job description...\")\n\n    with gr.Row():\n        output_resume = gr.Textbox(label=\"Tailored CV\", lines=10)\n        match_score = gr.Number(label=\"Match Score (%)\")\n\n    generate_btn = gr.Button(\"Tailor My CV\")\n\n    generate_btn.click(\n        fn=lambda r, j: (rewrite_cv(r, j), compute_similarity(r, j)),\n        inputs=[cv_input, job_input],\n        outputs=[output_resume, match_score]\n    )\n\napp.launch(share=True)\n","metadata":{"id":"yVgDcQ1OB6e8","trusted":true,"execution":{"iopub.status.busy":"2025-10-09T09:10:54.459730Z","iopub.execute_input":"2025-10-09T09:10:54.460450Z","iopub.status.idle":"2025-10-09T09:10:55.427367Z","shell.execute_reply.started":"2025-10-09T09:10:54.460425Z","shell.execute_reply":"2025-10-09T09:10:55.426748Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7864\n* Running on public URL: https://cb701d6fbe47e3359a.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://cb701d6fbe47e3359a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stderr","text":"The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\nSetting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2746a5d5080848adbd81627fb61183ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0c9badd53914a7c9b01d57f506714cb"}},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"","metadata":{"id":"g2Yw_CiEA9s7"}}]}